#%% md
# 熵
熵用于表示一个系统的稳定程度

如一个系统有两个可能发生的事件, 其概率分别为[0.5,0.5]

那么它将是一个非常不稳定的系统

如果概率为[0.99999,0.00001]那么它将是一个非常稳定的系统
 
熵（Entropy）是一个描述系统混乱程度或不确定性的概念，在物理学、信息论等领域中有不同的定义。这里列出几种常见的熵公式。

### 1. **热力学熵（热力学中的熵公式）**
在信息论中，熵用来衡量一个随机变量的不确定性或信息量。香农熵的公式为：

$$
H(X) = - \sum_{i=1}^{n} p(x_i) \log_b p(x_i)
$$

其中：
- $ H(X) $ 是随机变量 $ X $ 的熵，
- $ p(x_i) $ 是随机变量 $ X $ 取值 $ x_i $ 的概率，
- $ \log_b $ 是对数，常见的是以 2 为底的对数（即二进制对数），此时单位是比特（bit）。


#%%

import math
## Entropy
p = [0.2,0.4,0.4]
q = [0.2,0.3,0.5]
def entropy(p):
    return sum([-_p*math.log(_p) for _p in p])
#可见越稳定的系统,熵越小
print(f"entropy of p {entropy(p)},q {entropy(q)}")
#%%
## KL散度
## 两个概率分布间的差异
def kl_divergence(p,q):
    kl_p_q = sum([_p*math.log(_p/_q) for _p,_q in zip(p,q) if _p != 0.0])
    return kl_p_q
#
print(f"kl divergence of p||q {kl_divergence(p,q)} and q||p {kl_divergence(q,p)}")
#%%
## 交叉熵
def cross_entropy(p,q):
    ce  = sum([-_p*math.log(_q) for _p,_q in zip(p,q) if _p != 0.0])
    return ce
print(f"cross entropy of p||q {cross_entropy(p,q)}")

def cross_entropy(p,q):
    
# cross entropy loss
loss = cross_entropy_loss(p,q)
#%% md
