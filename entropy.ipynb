{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 熵\n",
    "在信息论中，熵用来衡量一个随机变量的不确定性或信息量。\n",
    "香农熵的公式为：\n",
    "\n",
    "$$\n",
    "H(X) = - \\sum_{i=1}^{n} p(x_i) \\log_b p(x_i)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $ H(X) $ 是随机变量 $ X $ 的熵，\n",
    "- $ p(x_i) $ 是随机变量 $ X $ 取值 $ x_i $ 的概率，\n",
    "\n"
   ],
   "id": "32ba2eb3ef9f1bcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import math\n",
    "## Entropy\n",
    "p = [0.2,0.4,0.4]\n",
    "q = [0.2,0.3,0.5]\n",
    "def entropy(p):\n",
    "    return sum([-_p*math.log(_p) for _p in p])\n",
    "#可见越稳定的系统,熵越小\n",
    "print(f\"entropy of p {entropy(p)},q {entropy(q)}\")"
   ],
   "id": "e993317f09593210"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    " \n",
    "# KL散度（Kullback-Leibler Divergence）\n",
    "也叫**相对熵**，是衡量两个概率分布 $ P $ 和 $ Q $ 之间差异的一个非对称指标。它衡量的是如果我们用 $ Q $ 来近似 $ P $，则信息损失有多大。\n",
    "\n",
    "### KL散度的公式：\n",
    "\n",
    "对于离散的随机变量，KL散度的公式为：\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\| Q) = \\sum_{i} p(x_i) \\log \\frac{p(x_i)}{q(x_i)}\n",
    "$$\n",
    "\n",
    "对于连续随机变量，KL散度的公式为：\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(P \\| Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} \\, dx\n",
    "$$\n",
    "\n",
    "### 公式解释：\n",
    "- $ P $ 和 $ Q $ 是两个概率分布。通常 $ P $ 代表真实分布，而 $ Q $ 代表近似分布。\n",
    "- $ p(x_i) $ 和 $ q(x_i) $ 分别是离散随机变量 $ X $ 在 $ x_i $ 上的概率密度函数或概率质量函数。\n",
    "\n",
    "### 重要特点：\n",
    "- **非负性**：KL散度总是大于或等于零，即 $ D_{\\text{KL}}(P \\| Q) \\geq 0 $。这是因为 $ \\log \\frac{p(x)}{q(x)} $ 是对数函数的特性（与Jensen不等式相关），并且当且仅当 $ P = Q $ 时，KL散度等于零。\n",
    "- **非对称性**：$ D_{\\text{KL}}(P \\| Q) \\neq D_{\\text{KL}}(Q \\| P) $，因此它不是一个对称的度量。"
   ],
   "id": "1707b16ee596ff77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## KL散度\n",
    "## 两个概率分布间的差异\n",
    "def kl_divergence(p,q):\n",
    "    kl_p_q = sum([_p*math.log(_p/_q) for _p,_q in zip(p,q) if _p != 0.0])\n",
    "    return kl_p_q\n",
    "#\n",
    "print(f\"kl divergence of p||q {kl_divergence(p,q)} and q||p {kl_divergence(q,p)}\")"
   ],
   "id": "b8fe08af807bda80"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " \n",
    "#交叉熵（Cross-Entropy）\n",
    "是信息论中的一个概念，通常用于衡量两个概率分布之间的差异。它是通过将一个分布的熵与另一个分布的交叉比较来计算的。在机器学习中，交叉熵常用于分类问题，尤其是在神经网络的训练过程中作为损失函数。\n",
    "\n",
    "### 交叉熵的公式\n",
    "\n",
    "对于离散随机变量，假设有两个概率分布 $ P $ 和 $ Q $，其中 $ P $ 是真实的概率分布，而 $ Q $ 是模型的预测概率分布。交叉熵定义为：\n",
    "\n",
    "$$\n",
    "H(P, Q) = - \\sum_{i} p(x_i) \\log q(x_i)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $ p(x_i) $ 是真实分布 $ P $ 在 $ x_i $ 上的概率\n",
    "- $ q(x_i) $ 是预测分布 $ Q $ 在 $ x_i $ 上的概率"
   ],
   "id": "bf831ab938a04575"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## 交叉熵\n",
    "def cross_entropy(p,q):\n",
    "    ce  = sum([-_p*math.log(_q) for _p,_q in zip(p,q) if _p != 0.0])\n",
    "    return ce\n",
    "print(f\"cross entropy of p||q {cross_entropy(p,q)}\")"
   ],
   "id": "e2bf8d3e70cb5890"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ac16e887fff2cc06"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
